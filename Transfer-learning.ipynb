{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Transfer Learning using Inception V3 Model and Convolutional Autoencoder on STL-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<!-- MarkdownTOC autolink=true autoanchor=true bracket=round -->\n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Inception V3 Model and how to perform Transfer Learning on it](#inception-intro)\n",
    "    - [Steps Followed to train](#inception-train)\n",
    "- [Semi-Supervised Transfer Learning with CAE and STL-10 dataset](#cae-stl)\n",
    "    - [Introduction](#cae-intro)\n",
    "    - [Convolutional Autoencoder (CAE)](#cae)\n",
    "    - [Transfer Learning using a Fully Connected Model for supervised learning](#cae-tl)\n",
    "- [Future Work](#future_work)\n",
    "- [Conclusion](#conclusion)\n",
    "- [References](#reference)\n",
    "\n",
    "<!-- /MarkdownTOC -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"> </a>\n",
    "## Introduction\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Deep Learning is becoming one of the most sought after skills in the current industry. Many companies are adopting AI into their produccts, emerging markets are found in Self-driving vehicles, image based product recognition and recommendation, face recognition for Snapchat filters, and many more. For all of these applications, the one major requirement is \"Data\"- Lots of it! To be precise, the AI models designed need Labeled data for training them. The hiccups in getting this dataset are below:\n",
    " \n",
    "1. Labeled data is hard to obtain due to the limited amount available\n",
    "2. Labeled data is expensive to acquire as a human (Amazon Mechanical Turk) must label them manually\n",
    "    \n",
    "Even if the labeled dataset were to be obtained, training a sophisticated model to the job would take a lot of time, money, and resources. The workarounds are simple.\n",
    "\n",
    "1. **To overcome the data scarcity**: Obtaining unlabeled images of almost any class from the internet is easy and the amount is abundant. Once the unlabeled dataset is obtained of similar, correlated classes, building and training a model just to learn features (not classify) from the images is the next step. After learning the weights of the model, the limited number of labeled dataset can then be fed to the model to classify the images.\n",
    "\n",
    "2. **To overcome the limitation of resources and time**: Pre-trained models such as Google's Inception model, and VGG16 Model are trained extensively on high power computers for weeks on ImageNet dataset and are capable of predicting 1000 classes given an image. Most of the predictions done by these models are accurate due to the depth of the model. These models can be modified slightly (discussed below) and can be made to classify images suitable for custom purposes with a far higher accuracy, with as little computation possible, and in a short amount of time. Again, this method also makes up for limited labeled dataset as the model does not need to learn weights of the images.\n",
    "\n",
    "The above listed methods are called \"Transfer Learning\". Transfer learning is helpful in mitigating the above two scenarios. And hence the motivation to take up this challenge. This notebook deals with Transfer Learning on Stanford's STL-10 dataset using Google Inception Model.\n",
    "\n",
    "The dataset can be found here: https://cs.stanford.edu/~acoates/stl10/\n",
    "\n",
    "The source codes can be found here: **INSERT LINK**\n",
    "\n",
    "Required Libraries: TensorFlow (v.1.2.1) and Numpy, and Inception V3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"inception-intro\"> </a>\n",
    "## Inception V3 Model and how to perform Transfer Learning on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Graph network of the Inception model is as shown in the Figure below <!-- MarkdownTOC autolink=true autoanchor=true bracket=round -->[[1]](#reference)<!-- /MarkdownTOC -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images_notebook/0.png \"Inception V3 Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception V3 model is a Deep Convolutional Network trained on the dataset of ImageNet and can classify an image into 1000 classes. There are many Convolution layers, Average Pooling, Max pooling, Dropout, finally a Fully-connected layer and Softmax layer for classification. The layer which is interesting for Transfer learning is the final pooling layer just before the Dense and Softmax layers. Because, until this layer, named 'pool_3', the model only does Feature extraction which is the computationally expensive part as this needs to be tuned as per the input images. \n",
    "\n",
    "The Penultimate two layers will be modified in this project to predict the 10 classes of STL-10 rather than ImageNet's 1000 classes. It can be observed that this slight modification yields a much better accuracy which a regular CNN would not be able to achieve due to less number of training examples.\n",
    "\n",
    "The Steps which are followed to achieve the Final results are as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"inception-train\"> </a>\n",
    "### Step 1: Download the Inception V3 model and extract the Model Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.platform import gfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "model='../inception/classify_image_graph_def.pb'\n",
    "\n",
    "def create_graph():\n",
    "    \n",
    "    '''\n",
    "    Function to extract GraphDef of Inception model.\n",
    "    Returns: Extracted GraphDef\n",
    "    \n",
    "    '''    \n",
    "    with tf.Session() as sess:\n",
    "        with gfile.FastGFile(model,'rb') as f:\n",
    "            graph_def=tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            _=tf.import_graph_def(graph_def,name='')\n",
    "            \n",
    "    return sess.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will load the TensorFlow's default graph with the Inception's graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Bottleneck the Training and Testing Images\n",
    "\n",
    "To perform Transfer Learning, we need to perform Bottlenecking. Bottleneck is a process where every image is fed to the Inception model and the output is taken from the intermediate Bottleneck layer in the graph rather than the output layer. In this case, we bottleneck every image until the pool_3 layer as that is the last layer performing feature extraction. In simpler terms, we \"freeze\" the model until this point. The code cell below shows a method that takes in a batch of images and outputs the bottlenecked version of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_pool3_features(sess,X_input):\n",
    "    \n",
    "    '''\n",
    "    Function to extract features for a given batch of images by\n",
    "    passing it through Inception model until pool_3 layer to get bottlenecks\n",
    "    \n",
    "    Args: Current Session, Batch of Images of size:batch_sizex96x96x3\n",
    "    Returns: Array of 2048 features extracted for every image by Inception\n",
    "    '''\n",
    "    n_train=X_input.shape[0]\n",
    "    pool3=sess.graph.get_tensor_by_name('pool_3:0')\n",
    "    x_pool3=[]\n",
    "    for i in range(n_train):\n",
    "        print (\"Iteration: \"+str(i))\n",
    "        features=sess.run(pool3,{'DecodeJpeg:0':X_input[i,:]})\n",
    "        x_pool3.append(np.squeeze(features))\n",
    "    return np.array(x_pool3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done for all the images (Train and Test) and they are saved as serialized Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bottleneck_data(sess):\n",
    "    \n",
    "    '''\n",
    "    Function to load STL data and process them to bottleneck them\n",
    "    \n",
    "    Args: TensorFlow session\n",
    "    '''\n",
    "    X_train, Y_train, X_test, Y_test=load_stl_data(one_hot=True)\n",
    "    bottleneck_pool3(sess,X_train, './X_train.npy')\n",
    "    bottleneck_pool3(sess,X_test, './X_test.npy')\n",
    "    np.save('./Y_train.npy',Y_train)\n",
    "    np.save('./Y_test.npy',Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step usually takes some time if the dataset is actually large (which partially defeats the purpose). Therefore, its best to save them on to the disk and make it a one-time process. This would now yield us a dataset of size from $None\\times96\\times96\\times3$ to $None\\times2048$. Which is a great deal of reduction data size. This means that, what was a 3-channel $96\\times96$ image is now feature extracted efficiently by Inception and is represented by a vector of size 2048. Thus paving way for a simple Neural Network for the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a Final Training Layer for Classification and Evaluation method\n",
    "\n",
    "With the Inception doing its job at feature extraction, and bottlenecking. It is now time for us to take over and perform the classification task between the 10 classes of STL-10 dataset. A fully connected model of structure 2048-1024-512-10 is constructed. To avoid overfitting due to low dimensions of data and less training data, Dropout layers are added to every layer with a retention probability of 0.75. The Intermediate layers have a Tanh activation function and the final, output layer comprises of Softmax activation. The Loss function used is Cross Entropy defined by:\n",
    "$$Loss = -\\frac{1}{n_{examples}} \\sum\\limits_x (y_{ground truth} \\ln y_{pred} + (1-y_{ground truth}) \\ln (1-y_{pred}))$$\n",
    "\n",
    "The Input dimensions are batch_size$\\times$2048 and output is an one-hot encoded representation of the 10 classes, therefore the output layer has 10 neurons. The Optimizer used is the regular Gradient Descent Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BOTTLENECK_TENSOR_NAME='pool_3'\n",
    "BOTTLENECK_TENSOR_SIZE=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_final_training_layer(class_count, final_tensor_name,\\\n",
    "                             ground_truth_tensor_name, learning_rate=1e-3):\n",
    "    \n",
    "    '''\n",
    "    Function to define the FC, Softmax classifier model to Classify the serialized\n",
    "    images. Has Gradient Descent Optimizer.\n",
    "    Includes Dropout layers and a 2048-1024-512-10 network\n",
    "    \n",
    "    Args: No. of classes, final tensor name of the FC network, \n",
    "    Ground Truth Tensor name, Learning rate for Optimizer\n",
    "    \n",
    "    Returns: Train Op and Cost of the model\n",
    "    '''\n",
    "    layers=[1024, 512, 10]\n",
    "    keep_prob=0.75\n",
    "    bottleneck_input=tf.placeholder(tf.float32,\\\n",
    "                                    shape=[None, BOTTLENECK_TENSOR_SIZE], name='BottleneckInput')\n",
    "    currentInput=bottleneck_input\n",
    "    n_input=BOTTLENECK_TENSOR_SIZE\n",
    "    for layer, output_size in enumerate(layers):\n",
    "        with tf.variable_scope('fc/layer{}'.format(layer)):\n",
    "            W=tf.get_variable(name='W', shape=[n_input, output_size], \\\n",
    "                              initializer=tf.random_normal_initializer(mean=0.0,stddev=0.01))\n",
    "            b=tf.get_variable(name='b',shape=[output_size],\\\n",
    "                              initializer=tf.constant_initializer([0]))\n",
    "            h=tf.matmul(currentInput,W)+b\n",
    "            n_input=output_size\n",
    "            if output_size!=layers[2]:\n",
    "                h=tf.nn.tanh(h,name='h')\n",
    "            else:\n",
    "                final_tensor=tf.nn.softmax(h, name=final_tensor_name)\n",
    "            h=tf.nn.dropout(h,keep_prob)\n",
    "            currentInput=h\n",
    "\n",
    "    Y=tf.placeholder(tf.float32, shape=[None,class_count],\\\n",
    "                     name=ground_truth_tensor_name)\n",
    "    cross_entropy=tf.nn.softmax_cross_entropy_with_logits(logits=h, labels=Y)\n",
    "    cost=tf.reduce_mean(cross_entropy)\n",
    "    train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    return train_step, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation and monitoring of the model's performance throughout training and the final test accuracy, the following method is employed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluation_step(graph, final_tensor_name, ground_truth_tensor_name):\n",
    "    \n",
    "    '''\n",
    "    Function to evaluate the performance of the model by calculating the \n",
    "    accuracy of prediction\n",
    "    \n",
    "    Args: Final Tensor and Ground Truth Tensor Name, TensorFlow Graph\n",
    "    \n",
    "    Return: Evaluation Tensor\n",
    "    '''\n",
    "    result_tensor=graph.get_tensor_by_name(ensure_port(final_tensor_name))\n",
    "    Y_tensor=graph.get_tensor_by_name(ensure_port(ground_truth_tensor_name))\n",
    "    correct_pred=tf.equal(tf.argmax(result_tensor,1),tf.argmax(Y_tensor,1))\n",
    "    \n",
    "    eval_step=tf.reduce_mean(tf.cast(correct_pred,'float'))\n",
    "    return eval_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training the Fully Connected model\n",
    "\n",
    "Final step in this process is training the model which is now going to classify the images. The number of epochs ran are 3500 with batch size of 250. One thing to note here is that, though the model got trained for 3500 epochs, which sounds like a lot of time, got completed in minutes, albeit running it on a GPU. The training accuracy and cost are printed every epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainer(sess, X_input, Y_input, X_test, Y_test):\n",
    "    \n",
    "    '''\n",
    "    Function to train the FC model with a Softmax activation for output layer\n",
    "    \n",
    "    Args: TensorFlow Session, Bottlenecked Images for training and testing\n",
    "    and corresponding labels\n",
    "    '''\n",
    "    ground_truth_tensor_name='ground_truth'\n",
    "    \n",
    "    # Define Batch size\n",
    "    mini_batch_size=250\n",
    "    n_train=X_input.shape[0]\n",
    "\n",
    "    graph=create_graph()\n",
    "    \n",
    "    # Get the train op and loss function\n",
    "    train_step,cross_entropy=add_final_training_layer\\\n",
    "    (n_classes, final_tensor_name, ground_truth_tensor_name, learning_rate)\n",
    "    # Intiliaze all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Get evaluation tensor\n",
    "    eval_step=evaluation_step(graph, \\\n",
    "    'fc/layer2/'+final_tensor_name, ground_truth_tensor_name)\n",
    "\n",
    "    # Get tensors for Input and Output    \n",
    "    bottleneck_input=graph.get_tensor_by_name(ensure_port('BottleneckInput'))\n",
    "    Y=graph.get_tensor_by_name(ensure_port(ground_truth_tensor_name))\n",
    "    \n",
    "    # Define number of epochs\n",
    "    epochs=3500\n",
    "    \n",
    "    # Perform training for number of epochs defined\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Shuffle the examples\n",
    "        shuffle=np.random.permutation(n_train)\n",
    "        \n",
    "        shuffle_X=X_input[shuffle,:]\n",
    "        shuffle_Y=Y_input[shuffle]\n",
    "        \n",
    "        # Perform batch training\n",
    "        for Xi, Yi in iterate_batches(shuffle_X, shuffle_Y, mini_batch_size):\n",
    "            sess.run(train_step, feed_dict={bottleneck_input:Xi, Y:Yi})\n",
    "        \n",
    "        # Print out model's performance after every epoch\n",
    "        train_accuracy, train_cross_entropy=\\\n",
    "        sess.run([eval_step,cross_entropy], \\\n",
    "                 feed_dict={bottleneck_input:X_input, Y:Y_input})\n",
    "        print (\"Epoch %d: Train accuracy:%0.2f, Cross Entropy:%0.2f\"\\\n",
    "               %(epoch,train_accuracy*100,train_cross_entropy))\n",
    "                \n",
    "    # Get the test accuracy after training is complete        \n",
    "    test_accuracy=sess.run(eval_step, \\\n",
    "                           feed_dict={bottleneck_input:X_test, Y:Y_test})\n",
    "    print('Final Test Accuracy:%0.2f' %(test_accuracy*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the trainer method after loading the saved bottleneck data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes=10\n",
    "\n",
    "X_train, Y_train, X_test, Y_test= load_bottleneck_data()\n",
    "\n",
    "final_tensor_name='final_result'\n",
    "learning_rate=0.001\n",
    "\n",
    "# Create TensorFlow session and train model \n",
    "sess=tf.InteractiveSession()\n",
    "trainer(sess, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model, on 5000 training samples achieved a very high train accuracy of 98.68% with a loss value of 0.03 and the Test accuracy on 8000 samples was 86.83%. It can be observed that when compared to state-of-the-art accuracy on STL-10 dataset, which is a semi-supervised work is at 74.3%, the accuracy obtained with a pre-trained network with very little computing work done, is much better and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cae-stl\"> </a>\n",
    "# Semi-Supervised Transfer Learning with CAE and STL-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cae-intro\"> </a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, Inception model did perform splendidly on the limited dataset available, it would be a bad choice for other uncorrelated dataset, say, speech. Transfer Learning with the pre-trained network always work best if the pre-trained network is fed correlated data. Let us take a minute to understand STL-10's intention to exist- Emphasize training semi-supervised! The dataset has 3 datasets:\n",
    "\n",
    "1. Train X and Train Y (5000 examples)\n",
    "2. Test X and Test Y (8000 examples)\n",
    "3. Unlabeled X (100000 examples)\n",
    "\n",
    "This is close to a real-world scenario. Getting unlabeled dataset from the internet is not hard. However, obtaining that many labeled data is hard, getting the exact data as the labeled data as unlabeled might be hard too. We need to make the best of what we have. Hence, STL-10.\n",
    "\n",
    "To give a brief information about how the dataset is advised to be used (Transfer Learning POV):\n",
    "\n",
    "* Perform Feature extraction on the Unlabeled dataset and learn weights. The unlabeled dataset does not contain the same images or same class of images as that of labeled (be it train or test) but it contains similar data\n",
    "* Perform supervised learning on this model with the labeled dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cae\"> </a>\n",
    "## Convolutional Autoencoder (CAE)\n",
    "\n",
    "In order to perform feature extraction from scratch, the model chosen to do is is the CAE. Convolution is used to extract information from related pixels in an image rather than considering a single pixel at a time. This gives a better spatial information at higher levels. CAE is a network which has two parts as shown below <!-- MarkdownTOC autolink=true autoanchor=true bracket=round -->[[2]](#reference)<!-- /MarkdownTOC -->: The convoluting encoder and de-convoluting decoder. \n",
    "![alt text](images_notebook/1.png \"Convolutional Autoencoder\")\n",
    "\n",
    "The main goal of CAE is image compression and lossy regeneration of it.\n",
    "\n",
    "In this project, since we only worry about the feature extraction, we only require the encoder layers of the model which gives us the compressed representation of the image. In order to accomodate the training of the CAE with a huge dataset, the model was trained on Google Cloud Platform for 300 epochs. Below cell will have the code snippet used to train. The model is saved every 10 epochs\n",
    "\n",
    "Requires: Google Cloud Platform account, (Money to train, if not on free trial), Google Cloud SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Python script which is the trainer task to train the CAE on Google Cloud \n",
    "Platform.\n",
    "\n",
    "Requires Google Cloud Platform account, Training data and scripts are to be \n",
    "placed inside Cloud Storage Bucket\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import argparse, os\n",
    "from StringIO import StringIO\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Batch size to be inputted\n",
    "batch_size=500\n",
    "# Filter window size for every layer\n",
    "filter_size=[4,4,4,4]\n",
    "\n",
    "\n",
    "def iterate_batches(x_in, batch_size):\n",
    "    '''\n",
    "    Function to randomly shuffle and yield batches for training\n",
    "    \n",
    "    Args: Unlabeled images and batch size\n",
    "    Returns: Batch of images, shuffled\n",
    "    \n",
    "    '''\n",
    "    new_perm=np.random.permutation(range(len(x_in)))\n",
    "    epoch_images=x_in[new_perm, ...]\n",
    "   \n",
    "    current_batch_id=0\n",
    "    while current_batch_id < len(x_in):\n",
    "        end=min(current_batch_id+batch_size,len(x_in))\n",
    "        batch_images={'images': epoch_images[current_batch_id:end]}\n",
    "        current_batch_id+=batch_size\n",
    "        yield batch_images['images']\n",
    "    \n",
    "                                 \n",
    "def train_model(train_file='../Unlabeled_X.npy', job_dir='./tmp/autoencoder', \\\n",
    "                output_dir='../output/', learning_rate=0.001, n_epochs=300, **args):\n",
    "    \n",
    "    '''\n",
    "    Function to train the CAE by taking in batches of images. Requires\n",
    "    arguments to be passed while initiating the job on GCP. Saves the model in \n",
    "    the Bucket every 10 epochs\n",
    "    \n",
    "    Args: Location of Training data (Cloud Storage Bucket), job-directory to \n",
    "    output logs of the job, learning rate and number of iterations for training\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    logs_path=job_dir+'/logs/'+datetime.now().isoformat()\n",
    "    output_file=os.path.join(output_dir,'saved-autoencoder-model')\n",
    "    logging.info('_____________________')\n",
    "    logging.info('Using Train File located at {}'.format(train_file))\n",
    "    logging.info('Using Logs_path located at {}'.format(logs_path))\n",
    "    logging.info('_____________________')\n",
    "    file_string=StringIO(file_io.read_file_to_string(train_file))\n",
    "    with tf.Graph().as_default():\n",
    "        sess=tf.InteractiveSession()\n",
    "        X_input=np.load(file_string)\n",
    "        idx=range(len(X_input))\n",
    "        \n",
    "        # Shuffle Data\n",
    "        rand_idxs=np.random.permutation(idx)\n",
    "        X_input=X_input[rand_idxs,...]\n",
    "\n",
    "\n",
    "        logging.info('Unlabeled Dataset loaded')\n",
    "        \n",
    "        features=X_input.shape[1]\n",
    "\n",
    "        # Number of filters for every layer\n",
    "        n_filters=[64,64,64,64]\n",
    "    \n",
    "        # Create placeholder for image tensor\n",
    "        X=tf.placeholder(tf.float32, shape=[None, features], name='X')\n",
    "        X_image_tensor=tf.reshape(X, [-1, 96, 96, 3])\n",
    "    \n",
    "        currentInput=X_image_tensor\n",
    "        n_input=currentInput.get_shape().as_list()[3]\n",
    "        Ws=[]\n",
    "        shapes=[]\n",
    "        \n",
    "        # Build a 4-layer convolutional encoder model by appending weights\n",
    "        # dimensions for decoder\n",
    "        for layer, output_size in enumerate(n_filters):\n",
    "            with tf.variable_scope(\"encoder/layer_{}\".format(layer)):\n",
    "                shapes.append(currentInput.get_shape().as_list())\n",
    "                W=tf.get_variable(name='W', shape=[filter_size[layer],\\\n",
    "                                                   filter_size[layer],\\\n",
    "                                                    n_input, output_size],\\\n",
    "                                                    initializer=\\\n",
    "                                                    tf.random_normal_initializer(mean=0.0,stddev=0.01))\n",
    "                b=tf.get_variable(name='b', shape=[output_size], initializer=\\\n",
    "                                  tf.constant_initializer([0]))\n",
    "                h=(tf.add(tf.nn.conv2d(currentInput, W, strides=[1,2,2,1],\\\n",
    "                               padding='SAME'),b))\n",
    "                h=tf.nn.relu(h,name='h')\n",
    "                currentInput=h\n",
    "                Ws.append(W)\n",
    "                n_input=output_size\n",
    "        \n",
    "        # Reverse weights matrix and shape matrix for decoder\n",
    "        Ws.reverse()\n",
    "        shapes.reverse()\n",
    "        n_filters.reverse()\n",
    "        n_filters=n_filters[1:]+[3]\n",
    "        \n",
    "        # Decoder for reconstruction of images\n",
    "        for layer, output_size in enumerate(shapes):\n",
    "            with tf.variable_scope('decoder/layer_{}'.format(layer)):\n",
    "                W=Ws[layer]\n",
    "                b = tf.Variable(tf.zeros([W.get_shape().as_list()[2]]))\n",
    "                output_shape=tf.stack([tf.shape(X)[0], \\\n",
    "                                       output_size[1],output_size[2],output_size[3]])\n",
    "                h=(tf.add(tf.nn.conv2d_transpose(currentInput, W, output_shape=output_shape, \\\n",
    "                                         strides=[1,2,2,1],padding='SAME'),b))\n",
    "                h=tf.nn.relu(h,name='h')\n",
    "                currentInput=h\n",
    "                \n",
    "        # Final Placeholder        \n",
    "        Y=currentInput\n",
    "        Y=tf.reshape(Y,[-1,96*96*3])\n",
    "        \n",
    "        cost=tf.reduce_mean(tf.reduce_mean(tf.squared_difference(X,Y),1))\n",
    "        optimizer=tf.train.AdamOptimizer(float(learning_rate)).minimize(cost)\n",
    "        \n",
    "        # Initiate Saver Instance\n",
    "        saver=tf.train.Saver()\n",
    "        \n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Start training\n",
    "        for i in range(int(n_epochs)):\n",
    "            for batch_img in iterate_batches(X_input, batch_size=batch_size):\n",
    "                sess.run(optimizer,feed_dict={X:batch_img})\n",
    "            # Every 10 epochs, report performance and save model graph and weights\n",
    "            if i%10==0:    \n",
    "                logging.info('Epoch:{0}, Cost={1}'.format(i, \\\n",
    "                             sess.run(cost, feed_dict={X: batch_img})))\n",
    "                saver.save(sess, output_file, global_step=0)\n",
    "                logging.info('Model Saved')\n",
    "\n",
    "                \n",
    "                \n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-file', help='GCS or local paths to train data',\\\n",
    "                        required=True)\n",
    "    parser.add_argument('--job-dir', help='GCS location to write \\\n",
    "    checkpoints and export models', required=True)\n",
    "    parser.add_argument('--output_dir', help='GCS location \\\n",
    "    to write model', required=True)\n",
    "    parser.add_argument('--learning-rate', help='Learning Rate', required=True)\n",
    "    parser.add_argument('--n-epochs', help='Number of epochs', required=True)\n",
    "    \n",
    "    args=parser.parse_args()\n",
    "    arguments=args.__dict__\n",
    "    \n",
    "    train_model(**arguments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script was run with a standard-GPU (Single Tesla K80 GPU) tier on the GCP's Cloud ML Engine API for training. There are 4 convolutional layers with filter window size of 4$\\times$4 for every filter in every layer. Below are the montage of images of every layer of encoder for an image that is obtained after training. \n",
    "\n",
    "<img src=\"./images_notebook/2.png\" width=\"350\" title=\"Encoder Layer 1\" hspace=\"50\"/>\n",
    "<img src=\"./images_notebook/3.png\" width=\"350\" title=\"Encoder Layer 2\" hspace=\"50\"/>\n",
    "<img src=\"./images_notebook/4.png\" width=\"350\" title=\"Encoder Layer 3\" hspace=\"50\"/>\n",
    "\n",
    "The above figures show the montage of filter outputs after the ReLU functions of every encoder layer. Notice how the heat map is red for that particular filter's activation features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the original image fed to the network is as shown below:\n",
    "\n",
    "<img src=\"./images_notebook/5.png\" width=\"350\" title=\"Original Image\" hspace=\"50\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cae-tl\"> </a>\n",
    "## Transfer Learning using a Fully Connected Model for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model with a fairly decent performance given the resources, the second step is the same as Inception but with slight modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_graph(sess):\n",
    "    \n",
    "    '''\n",
    "    Function to extract Graph and model from the trained CAE.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    saver=tf.train.import_meta_graph(model_meta)\n",
    "    saver.restore(sess, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the TensorFlow default graph with our model's graph and weights. Unlike Inception's pool_3, we choose our bottleneck layer to be the last encoder layer, the 'encoder/layer_3/h' which is the ReLU output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(sess, X_input):\n",
    "    \n",
    "    '''\n",
    "    Function to extract features for a given batch of images by\n",
    "    passing it through CAE model until the layer 3 of ReLu of encoder to get bottlenecks\n",
    "    \n",
    "    Args: Current Session, Images\n",
    "    Returns: Array of 2304 features extracted for every image by Inception\n",
    "    '''\n",
    "\n",
    "    encoder_relu=sess.graph.get_tensor_by_name('encoder/layer_3/h:0')\n",
    "    features=sess.run(encoder_relu, feed_dict={'X:0':X_input})\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest remain the same, i.e., saving bottlenecks, addition of final layer, evaluation step definition, and finally train the Fully connected model with the training examples. Below is the Terminal output of training the Fully Connected model. For clarity, every 50 epochs data is displayed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "C:\\Users\\Sharath\\Documents\\Machine Learning\\Startup.ML\\gcloud\\transfer learning>python fc_train.py\n",
    "2017-08-17 10:53:33.952634: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-08-17 10:53:33.952990: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-08-17 10:53:33.954768: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-08-17 10:53:33.955172: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-08-17 10:53:33.956779: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-08-17 10:53:33.958236: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-08-17 10:53:33.959671: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-08-17 10:53:33.960981: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-08-17 10:53:35.855224: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:940] Found device 0 with properties:\n",
    "name: GeForce GT 740M\n",
    "major: 3 minor: 5 memoryClockRate (GHz) 1.0325\n",
    "pciBusID 0000:0a:00.0\n",
    "Total memory: 2.00GiB\n",
    "Free memory: 1.67GiB\n",
    "2017-08-17 10:53:35.855834: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:961] DMA: 0\n",
    "2017-08-17 10:53:35.860241: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   Y\n",
    "2017-08-17 10:53:35.862274: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 740M, pci bus id: 0000:0a:00.0)\n",
    "Epoch 0: Train accuracy:13.04, Cross Entropy:2.30\n",
    "\n",
    "Epoch 50: Train accuracy:35.78, Cross Entropy:1.81\n",
    "Epoch 100: Train accuracy:41.82, Cross Entropy:1.66\n",
    "Epoch 150: Train accuracy:59.64, Cross Entropy:1.35\n",
    "Epoch 200: Train accuracy:68.78, Cross Entropy:1.14\n",
    "Epoch 250: Train accuracy:63.56, Cross Entropy:1.26\n",
    "Epoch 300: Train accuracy:82.28, Cross Entropy:0.74\n",
    "Epoch 350: Train accuracy:77.56, Cross Entropy:0.88\n",
    "Epoch 400: Train accuracy:91.80, Cross Entropy:0.49\n",
    "Epoch 450: Train accuracy:85.94, Cross Entropy:0.62\n",
    "Epoch 500: Train accuracy:92.42, Cross Entropy:0.46\n",
    "Epoch 550: Train accuracy:97.96, Cross Entropy:0.33\n",
    "\n",
    "Final Test Accuracy for CAE Transfer Learning:43.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy obtained was 43.63% which is quite poor when compared to the Inception version of the project. This is due to the CAE's simplicity in extracting the features and high training cost due to the poor reconstruction. But this opens the door to create our own model to pre-train using the related unlabeled dataset we could obtain and apply supervised learning to the little training examples we might possess. Also, the State-of-the-art (SWWAE<!-- MarkdownTOC autolink=true autoanchor=true bracket=round -->[[4]](#reference)<!-- /MarkdownTOC -->) accuracy obtained on any semi-supervised work done on STL-10 as per data on <!-- MarkdownTOC autolink=true autoanchor=true bracket=round -->[[3]](#reference)<!-- /MarkdownTOC --> is 74.33% and the last on the list<!-- MarkdownTOC autolink=true autoanchor=true bracket=round -->[[5]](#reference)<!-- /MarkdownTOC --> is having accuracy of 58.28%. Comparitively, the model has a decent performance. The performance can be more enhanced by having a deeper model and more filters to extract features from the layers in the CAE. The Graph below shows the accuracy comparisons between the model designed using pre-trained Inception model, the SWWAE, Pooling Invariant Image Feature Learning, and the CAE.\n",
    "\n",
    "<img src=\"./images_notebook/6.png\" width=\"500\" title=\"Graph of Comparison\" hspace=\"50\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"future_work\"> </a>\n",
    "## Future Work\n",
    "\n",
    "The Inception version of the model can be made to work better by bottlenecking earlier layers rather than the final 'pool_3' layer. This would render more work on our end to fine-tune the model. One other possibility in the case of STL-10 is Data Augmentation to increase the number of training examples and prevent chances of over fitting. The key interest would be to improve the accuracy of the CAE designed. Increasing the complexity of the model by adding deeper encoder/decoder layers, increasing the number of Conv Filters per layer and also the window size for every filter would render a better feature extraction model. However, the caviet here would be requirement for a powerful machine and longer training time. Since the CAE was trained on the cloud, the training might require a powerful tier of compute engine, rendering the process expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"> </a>\n",
    "## Conclusion\n",
    "\n",
    "In this work, STL-10 Dataset was used to perform Transfer Learning on Pre-trained networks such as Inception V3 model. Later, we built our own feature extraction model using CAE using STL's unlabeled dataset and the last layer of Encoder layer was extracted to perform the supervised learning on the labeled dataset by bottlenecking the images into None$\\times$2304 thereby making it a simpler representation of the dataset. The bottlenecks were then trained using a simple Fully connected network to yield a test accuracy of 43.63%. One interesting thing to note here is that, the choice of model depends on the user's needs. In the case of Inception, we were lucky to have images as examples since the Inception was trained on the same. Any other data would have yielded poor performance on the Inception. However, since we had a large dataset of unlabeled dataset of similar images, we were able to train a CAE which is an unsupervised convolutional model used to generate lossy images given an image, we performed transfer learning on this model to obtain a fairly decent accuracy. This approach gives the user freedom to design his/her own pre-trained network no matter what the data is. The only 3 requirements being, possession of a large unlabeled dataset, Compute heavy engine, and Time. It is up to the user to use pre-trained networks like Inception or VGGNet or build their own given the circumstances they are in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"reference\"> </a>\n",
    "## References\n",
    "\n",
    "1. Google Inception Graph. [Image Courtesy: www.research.googleblog.com]\n",
    "2. Convolutional Autoencoder Structure. [Image Courtesy: www.researchgate.net]\n",
    "3. Classification Dataset Results- Discover the current State of the art in Objects classification URL:http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#53544c2d3130\n",
    "4. Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun, \"Stacked What-Where Auto-encoders\" [arXiv:1506.02351 [stat.ML]]\n",
    "5. Yangqing Jia, et.al. \"Pooling-Invariant Image Feature Learning\"\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
